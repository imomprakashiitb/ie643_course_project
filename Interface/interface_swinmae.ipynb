{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Swin_MAE_Interface\n",
        "\n"
      ],
      "metadata": {
        "id": "hplpTGoI0oLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependent Scripts"
      ],
      "metadata": {
        "id": "IlTUQmKN38On"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/utils/pos_embed.py', 'w') as f:\n",
        "    f.write('''\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype= float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000 ** omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out)  # (M, D/2)\n",
        "    emb_cos = np.cos(out)  # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "def interpolate_pos_embed(model, checkpoint_model):\n",
        "    if 'pos_embed' in checkpoint_model:\n",
        "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        if orig_size != new_size:\n",
        "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
        "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "            # only the position tokens are interpolated\n",
        "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "            pos_tokens = torch.nn.functional.interpolate(\n",
        "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "            checkpoint_model['pos_embed'] = new_pos_embed\n",
        "''')"
      ],
      "metadata": {
        "id": "KBNTKLRvFypn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/utils/misc.py', 'w') as f:\n",
        "    f.write('''\n",
        "\n",
        "\n",
        "import builtins\n",
        "import datetime\n",
        "import os\n",
        "import time\n",
        "from collections import defaultdict, deque\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "#from torch._six import inf\n",
        "from torch import inf\n",
        "\n",
        "\n",
        "class SmoothedValue(object):\n",
        "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
        "    window or the global series average.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=20, fmt=None):\n",
        "        if fmt is None:\n",
        "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
        "        self.deque = deque(maxlen=window_size)\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.deque.append(value)\n",
        "        self.count += n\n",
        "        self.total += value * n\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        \"\"\"\n",
        "        Warning: does not synchronize the deque!\n",
        "        \"\"\"\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return\n",
        "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(t)\n",
        "        t = t.tolist()\n",
        "        self.count = int(t[0])\n",
        "        self.total = t[1]\n",
        "\n",
        "    @property\n",
        "    def median(self):\n",
        "        d = torch.tensor(list(self.deque))\n",
        "        return d.median().item()\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
        "        return d.mean().item()\n",
        "\n",
        "    @property\n",
        "    def global_avg(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    @property\n",
        "    def max(self):\n",
        "        return max(self.deque)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.deque[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fmt.format(\n",
        "            median=self.median,\n",
        "            avg=self.avg,\n",
        "            global_avg=self.global_avg,\n",
        "            max=self.max,\n",
        "            value=self.value)\n",
        "\n",
        "\n",
        "class MetricLogger(object):\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if v is None:\n",
        "                continue\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
        "            type(self).__name__, attr))\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(\n",
        "                \"{}: {}\".format(name, str(meter))\n",
        "            )\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = ''\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
        "        log_msg = [\n",
        "            header,\n",
        "            '[{0' + space_fmt + '}/{1}]',\n",
        "            'eta: {eta}',\n",
        "            '{meters}',\n",
        "            'time: {time}',\n",
        "            'data: {data}'\n",
        "        ]\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg.append('max mem: {memory:.0f}')\n",
        "        log_msg = self.delimiter.join(log_msg)\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time),\n",
        "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
        "                else:\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time)))\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
        "            header, total_time_str, total_time / len(iterable)))\n",
        "\n",
        "\n",
        "def setup_for_distributed(is_master):\n",
        "    \"\"\"\n",
        "    This function disables printing when not in master process\n",
        "    \"\"\"\n",
        "    builtin_print = builtins.print\n",
        "\n",
        "    def print(*args, **kwargs):\n",
        "        force = kwargs.pop('force', False)\n",
        "        force = force or (get_world_size() > 8)\n",
        "        if is_master or force:\n",
        "            now = datetime.datetime.now().time()\n",
        "            builtin_print('[{}] '.format(now), end='')  # print with time stamp\n",
        "            builtin_print(*args, **kwargs)\n",
        "\n",
        "    builtins.print = print\n",
        "\n",
        "\n",
        "def is_dist_avail_and_initialized():\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def save_on_master(*args, **kwargs):\n",
        "    if is_main_process():\n",
        "        torch.save(*args, **kwargs)\n",
        "\n",
        "\n",
        "def init_distributed_mode(args):\n",
        "    if args.dist_on_itp:\n",
        "        args.rank = int(os.environ['OMPI_COMM_WORLD_RANK'])\n",
        "        args.world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])\n",
        "        args.gpu = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n",
        "        args.dist_url = \"tcp://%s:%s\" % (os.environ['MASTER_ADDR'], os.environ['MASTER_PORT'])\n",
        "        os.environ['LOCAL_RANK'] = str(args.gpu)\n",
        "        os.environ['RANK'] = str(args.rank)\n",
        "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
        "        # [\"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\"]\n",
        "    elif 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
        "        args.rank = int(os.environ[\"RANK\"])\n",
        "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
        "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
        "    elif 'SLURM_PROCID' in os.environ:\n",
        "        args.rank = int(os.environ['SLURM_PROCID'])\n",
        "        args.gpu = args.rank % torch.cuda.device_count()\n",
        "    else:\n",
        "        print('Not using distributed mode')\n",
        "        setup_for_distributed(is_master=True)  # hack\n",
        "        args.distributed = False\n",
        "        return\n",
        "\n",
        "    args.distributed = True\n",
        "\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "    args.dist_backend = 'nccl'\n",
        "    print('| distributed init (rank {}): {}, gpu {}'.format(\n",
        "        args.rank, args.dist_url, args.gpu), flush=True)\n",
        "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                         world_size=args.world_size, rank=args.rank)\n",
        "    torch.distributed.barrier()\n",
        "    setup_for_distributed(args.rank == 0)\n",
        "\n",
        "\n",
        "class NativeScalerWithGradNormCount:\n",
        "    state_dict_key = \"amp_scaler\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
        "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
        "        if update_grad:\n",
        "            if clip_grad is not None:\n",
        "                assert parameters is not None\n",
        "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
        "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
        "            else:\n",
        "                self._scaler.unscale_(optimizer)\n",
        "                norm = get_grad_norm_(parameters)\n",
        "            self._scaler.step(optimizer)\n",
        "            self._scaler.update()\n",
        "        else:\n",
        "            norm = None\n",
        "        return norm\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self._scaler.state_dict()\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self._scaler.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n",
        "    if isinstance(parameters, torch.Tensor):\n",
        "        parameters = [parameters]\n",
        "    parameters = [p for p in parameters if p.grad is not None]\n",
        "    norm_type = float(norm_type)\n",
        "    if len(parameters) == 0:\n",
        "        return torch.tensor(0.)\n",
        "    device = parameters[0].grad.device\n",
        "    if norm_type == inf:\n",
        "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
        "    else:\n",
        "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
        "    return total_norm\n",
        "\n",
        "\n",
        "def save_model(args, epoch, model, model_without_ddp, optimizer, loss_scaler):\n",
        "    output_dir = Path(args.output_dir)\n",
        "    epoch_name = str(epoch)\n",
        "    if loss_scaler is not None:\n",
        "        checkpoint_paths = [output_dir / ('checkpoint-%s.pth' % epoch_name)]\n",
        "        for checkpoint_path in checkpoint_paths:\n",
        "            to_save = {\n",
        "                'model': model_without_ddp.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'scaler': loss_scaler.state_dict(),\n",
        "                'args': args,\n",
        "            }\n",
        "\n",
        "            save_on_master(to_save, checkpoint_path)\n",
        "    else:\n",
        "        client_state = {'epoch': epoch}\n",
        "        model.save_checkpoint(save_dir=args.output_dir, tag=\"checkpoint-%s\" % epoch_name, client_state=client_state)\n",
        "\n",
        "\n",
        "def load_model(args, model_without_ddp):\n",
        "    model_dict = model_without_ddp.state_dict()\n",
        "    if args.checkpoint_encoder:\n",
        "        print(f'Use encoder weights: {args.checkpoint_encoder}')\n",
        "        checkpoint_encoder = torch.load(args.checkpoint_encoder, map_location='cpu')['model']\n",
        "        for key in list(checkpoint_encoder.keys()):\n",
        "            if key not in model_dict:\n",
        "                del checkpoint_encoder[key]\n",
        "    else:\n",
        "        checkpoint_encoder = {}\n",
        "\n",
        "    if args.checkpoint_decoder:\n",
        "        print(f'Use decoder weights: {args.checkpoint_decoder}')\n",
        "        checkpoint_decoder = torch.load(args.checkpoint_decoder, map_location='cpu')['model']\n",
        "        for key in list(checkpoint_decoder.keys()):\n",
        "            if (not key.startswith('decoder')) or (key not in model_dict):\n",
        "                del checkpoint_decoder[key]\n",
        "        for key in list(checkpoint_decoder.keys()):\n",
        "            if key in model_dict:\n",
        "                if checkpoint_decoder[key].shape != model_dict[key].shape:\n",
        "                    print(f\"Delete: '{key}'; \"\n",
        "                          f\"Weight shape: {checkpoint_decoder[key].shape}; \"\n",
        "                          f\"Model shape: {model_dict[key].shape}\")\n",
        "                    del checkpoint_decoder[key]\n",
        "    else:\n",
        "        checkpoint_decoder = {}\n",
        "\n",
        "    checkpoint = checkpoint_encoder\n",
        "    for key in list(checkpoint_decoder):\n",
        "        checkpoint[key] = checkpoint_decoder[key]\n",
        "\n",
        "    if checkpoint:\n",
        "        result = model_without_ddp.load_state_dict(checkpoint, strict=False)\n",
        "        print(result)\n",
        "\n",
        "\n",
        "def all_reduce_mean(x):\n",
        "    world_size = get_world_size()\n",
        "    if world_size > 1:\n",
        "        x_reduce = torch.tensor(x).cuda()\n",
        "        dist.all_reduce(x_reduce)\n",
        "        x_reduce /= world_size\n",
        "        return x_reduce.item()\n",
        "    else:\n",
        "        return x\n",
        "''')"
      ],
      "metadata": {
        "id": "JCMA1SKLGMvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/utils/lr_sched.py', 'w') as f:\n",
        "    f.write('''\n",
        "import math\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n",
        "    if epoch < args.warmup_epochs:\n",
        "        lr = args.lr * epoch / args.warmup_epochs\n",
        "    else:\n",
        "        lr = args.min_lr + (args.lr - args.min_lr) * 0.5 * \\\n",
        "             (1. + math.cos(math.pi * (epoch - args.warmup_epochs) / (args.epochs - args.warmup_epochs)))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        if \"lr_scale\" in param_group:\n",
        "            param_group[\"lr\"] = lr * param_group[\"lr_scale\"]\n",
        "        else:\n",
        "            param_group[\"lr\"] = lr\n",
        "    return lr\n",
        "''')"
      ],
      "metadata": {
        "id": "PVho3W3wGVY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/utils/engine_pretrain.py', 'w') as f:\n",
        "    f.write('''\n",
        "import math\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "\n",
        "import utils.misc as misc\n",
        "import utils.lr_sched as lr_sched\n",
        "\n",
        "\n",
        "def train_one_epoch(model: torch.nn.Module,\n",
        "                    data_loader, optimizer: torch.optim.Optimizer,\n",
        "                    device: torch.device, epoch: int, loss_scaler,\n",
        "                    log_writer=None,\n",
        "                    args=None):\n",
        "    model.train(True)\n",
        "    metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', misc.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "    print_freq = 10\n",
        "\n",
        "    accum_iter = args.accum_iter\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if log_writer is not None:\n",
        "        print('log_dir: {}'.format(log_writer.log_dir))\n",
        "\n",
        "    for data_iter_step, samples in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
        "        #print(samples['image'])\n",
        "        # we use a per iteration (instead of per epoch) lr scheduler\n",
        "        if data_iter_step % accum_iter == 0:\n",
        "            lr_sched.adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, args)\n",
        "\n",
        "        input_image = samples['image']\n",
        "        batch1 = torch.einsum('nchw->nhwc', input_image)\n",
        "        stacked_img = np.stack((batch1[:,:,:,0],)*3, axis=-1)\n",
        "        stacked_img = torch.einsum('nhwc->nchw', torch.from_numpy(stacked_img)).to(device, non_blocking=True)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            loss, pred, mask = model(stacked_img)\n",
        "            y1 = model.unpatchify(pred)\n",
        "        loss_value = loss.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            sys.exit(1)\n",
        "\n",
        "        loss /= accum_iter\n",
        "        loss_scaler(loss, optimizer, parameters=model.parameters(),\n",
        "                    update_grad=(data_iter_step + 1) % accum_iter == 0)\n",
        "        if (data_iter_step + 1) % accum_iter == 0:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        metric_logger.update(loss=loss_value)\n",
        "\n",
        "        lr = optimizer.param_groups[0][\"lr\"]\n",
        "        metric_logger.update(lr=lr)\n",
        "\n",
        "        loss_value_reduce = misc.all_reduce_mean(loss_value)\n",
        "        if log_writer is not None and (data_iter_step + 1) % accum_iter == 0:\n",
        "            \"\"\" We use epoch_1000x as the x-axis in tensorboard.\n",
        "            This calibrates different curves when batch size changes.\n",
        "            \"\"\"\n",
        "            epoch_1000x = int((data_iter_step / len(data_loader) + epoch) * 1000)\n",
        "            log_writer.add_scalar('train_loss', loss_value_reduce, epoch_1000x)\n",
        "            log_writer.add_scalar('lr', lr, epoch_1000x)\n",
        "    #After completion of all steps in one epoch, i.e. in last step we will have one entry\n",
        "    grid_batch_input = ((torchvision.utils.make_grid(stacked_img[0:10,:,:,:])))\n",
        "    log_writer.add_image(f\"input_image/{epoch}\", grid_batch_input,epoch)\n",
        "    grid_batch_output = ((torchvision.utils.make_grid(y1[0:10,:,:,:])))\n",
        "    log_writer.add_image(f'output_image/{epoch}', grid_batch_output ,epoch)\n",
        "    ####log_writer.add_image(f'output_image', grid_batch_output ,epoch)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "''')"
      ],
      "metadata": {
        "id": "JY4K1GdmGVRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/swin_unet.py', 'w') as f:\n",
        "    f.write('''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "\n",
        "from einops import rearrange\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob: float = 0.):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.drop_prob == 0. or not self.training:\n",
        "            return x\n",
        "\n",
        "        keep_prob = 1 - self.drop_prob\n",
        "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "        random_tensor.floor_()\n",
        "        x = x.div(keep_prob) * random_tensor\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, patch_size: int = 4, in_c: int = 3, embed_dim: int = 96, norm_layer: nn.Module = None):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=(patch_size,) * 2, stride=(patch_size,) * 2)\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
        "\n",
        "    def padding(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        _, _, H, W = x.shape\n",
        "        if H % self.patch_size != 0 or W % self.patch_size != 0:\n",
        "            x = func.pad(x, (0, self.patch_size - W % self.patch_size,\n",
        "                             0, self.patch_size - H % self.patch_size,\n",
        "                             0, 0))\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.padding(x)\n",
        "        x = self.proj(x)\n",
        "        x = rearrange(x, 'B C H W -> B H W C')\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, dim: int, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.norm = norm_layer(4 * dim)\n",
        "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def padding(x: torch.Tensor) -> torch.Tensor:\n",
        "        _, H, W, _ = x.shape\n",
        "\n",
        "        if H % 2 == 1 or W % 2 == 1:\n",
        "            x = func.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def merging(x: torch.Tensor) -> torch.Tensor:\n",
        "        x0 = x[:, 0::2, 0::2, :]\n",
        "        x1 = x[:, 1::2, 0::2, :]\n",
        "        x2 = x[:, 0::2, 1::2, :]\n",
        "        x3 = x[:, 1::2, 1::2, :]\n",
        "        x = torch.cat([x0, x1, x2, x3], -1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.padding(x)\n",
        "        x = self.merging(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchExpanding(nn.Module):\n",
        "    def __init__(self, dim: int, norm_layer=nn.LayerNorm):\n",
        "        super(PatchExpanding, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.expand = nn.Linear(dim, 2 * dim, bias=False)\n",
        "        self.norm = norm_layer(dim // 2)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.expand(x)\n",
        "        x = rearrange(x, 'B H W (P1 P2 C) -> B (H P1) (W P2) C', P1=2, P2=2)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FinalPatchExpanding(nn.Module):\n",
        "    def __init__(self, dim: int, norm_layer=nn.LayerNorm):\n",
        "        super(FinalPatchExpanding, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.expand = nn.Linear(dim, 16 * dim, bias=False)\n",
        "        self.norm = norm_layer(dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.expand(x)\n",
        "        x = rearrange(x, 'B H W (P1 P2 C) -> B (H P1) (W P2) C', P1=4, P2=4)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features: int, hidden_features: int = None, out_features: int = None,\n",
        "                 act_layer=nn.GELU, drop: float = 0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.drop1 = nn.Dropout(drop)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop2 = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim: int, window_size: int, num_heads: int, qkv_bias: Optional[bool] = True,\n",
        "                 attn_drop: Optional[float] = 0., proj_drop: Optional[float] = 0., shift: bool = False):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = (dim // num_heads) ** -0.5\n",
        "\n",
        "        if shift:\n",
        "            self.shift_size = window_size // 2\n",
        "        else:\n",
        "            self.shift_size = 0\n",
        "\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size - 1) ** 2, num_heads))\n",
        "        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "\n",
        "        coords_size = torch.arange(self.window_size)\n",
        "        coords = torch.stack(torch.meshgrid([coords_size, coords_size], indexing=\"ij\"))\n",
        "        coords_flatten = torch.flatten(coords, 1)\n",
        "\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
        "        relative_coords[:, :, 0] += self.window_size - 1\n",
        "        relative_coords[:, :, 1] += self.window_size - 1\n",
        "        relative_coords[:, :, 0] *= 2 * self.window_size - 1\n",
        "        relative_position_index = relative_coords.sum(-1)\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def window_partition(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        _, H, W, _ = x.shape\n",
        "\n",
        "        x = rearrange(x, 'B (Nh Mh) (Nw Mw) C -> (B Nh Nw) Mh Mw C', Nh=H // self.window_size, Nw=W // self.window_size)\n",
        "        return x\n",
        "\n",
        "    def create_mask(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        _, H, W, _ = x.shape\n",
        "\n",
        "        assert H % self.window_size == 0 and W % self.window_size == 0, \"H or W is not divisible by window_size\"\n",
        "\n",
        "        img_mask = torch.zeros((1, H, W, 1), device=x.device)\n",
        "        h_slices = (slice(0, -self.window_size),\n",
        "                    slice(-self.window_size, -self.shift_size),\n",
        "                    slice(-self.shift_size, None))\n",
        "        w_slices = (slice(0, -self.window_size),\n",
        "                    slice(-self.window_size, -self.shift_size),\n",
        "                    slice(-self.shift_size, None))\n",
        "        cnt = 0\n",
        "        for h in h_slices:\n",
        "            for w in w_slices:\n",
        "                img_mask[:, h, w, :] = cnt\n",
        "                cnt += 1\n",
        "\n",
        "        mask_windows = self.window_partition(img_mask)\n",
        "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
        "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
        "\n",
        "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
        "        return attn_mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, H, W, _ = x.shape\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "            mask = self.create_mask(x)\n",
        "        else:\n",
        "            mask = None\n",
        "\n",
        "        x = self.window_partition(x)\n",
        "        Bn, Mh, Mw, _ = x.shape\n",
        "        x = rearrange(x, 'Bn Mh Mw C -> Bn (Mh Mw) C')\n",
        "        qkv = rearrange(self.qkv(x), 'Bn L (T Nh P) -> T Bn Nh L P', T=3, Nh=self.num_heads)\n",
        "        q, k, v = qkv.unbind(0)\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            self.window_size ** 2, self.window_size ** 2, -1)\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(Bn // nW, nW, self.num_heads, Mh * Mw, Mh * Mw) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, Mh * Mw, Mh * Mw)\n",
        "        attn = self.softmax(attn)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = attn @ v\n",
        "        x = rearrange(x, 'Bn Nh (Mh Mw) C -> Bn Mh Mw (Nh C)', Mh=Mh)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        x = rearrange(x, '(B Nh Nw) Mh Mw C -> B (Nh Mh) (Nw Mw) C', Nh=H // Mh, Nw=H // Mw)\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        return x\n",
        "\n",
        "\n",
        "class SwinTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, window_size=7, shift=False, mlp_ratio=4., qkv_bias=True,\n",
        "                 drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = WindowAttention(dim, window_size=window_size, num_heads=num_heads, qkv_bias=qkv_bias,\n",
        "                                    attn_drop=attn_drop, proj_drop=drop, shift=shift)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_copy = x\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        x = self.attn(x)\n",
        "        x = self.drop_path(x)\n",
        "        x = x + x_copy\n",
        "\n",
        "        x_copy = x\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        x = self.mlp(x)\n",
        "        x = self.drop_path(x)\n",
        "        x = x + x_copy\n",
        "        return x\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, index: int, embed_dim: int = 96, window_size: int = 7, depths: tuple = (2, 2, 6, 2),\n",
        "                 num_heads: tuple = (3, 6, 12, 24), mlp_ratio: float = 4., qkv_bias: bool = True,\n",
        "                 drop_rate: float = 0., attn_drop_rate: float = 0., drop_path: float = 0.1,\n",
        "                 norm_layer=nn.LayerNorm, patch_merging: bool = True):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        depth = depths[index]\n",
        "        dim = embed_dim * 2 ** index\n",
        "        num_head = num_heads[index]\n",
        "\n",
        "        dpr = [rate.item() for rate in torch.linspace(0, drop_path, sum(depths))]\n",
        "        drop_path_rate = dpr[sum(depths[:index]):sum(depths[:index + 1])]\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            SwinTransformerBlock(\n",
        "                dim=dim,\n",
        "                num_heads=num_head,\n",
        "                window_size=window_size,\n",
        "                shift=False if (i % 2 == 0) else True,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=drop_path_rate[i],\n",
        "                norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "\n",
        "        if patch_merging:\n",
        "            self.downsample = PatchMerging(dim=embed_dim * 2 ** index, norm_layer=norm_layer)\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.blocks:\n",
        "            x = layer(x)\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BasicBlockUp(nn.Module):\n",
        "    def __init__(self, index: int, embed_dim: int = 96, window_size: int = 7, depths: tuple = (2, 2, 6, 2),\n",
        "                 num_heads: tuple = (3, 6, 12, 24), mlp_ratio: float = 4., qkv_bias: bool = True,\n",
        "                 drop_rate: float = 0., attn_drop_rate: float = 0., drop_path: float = 0.1,\n",
        "                 patch_expanding: bool = True, norm_layer=nn.LayerNorm):\n",
        "        super(BasicBlockUp, self).__init__()\n",
        "        index = len(depths) - index - 2\n",
        "        depth = depths[index]\n",
        "        dim = embed_dim * 2 ** index\n",
        "        num_head = num_heads[index]\n",
        "\n",
        "        dpr = [rate.item() for rate in torch.linspace(0, drop_path, sum(depths))]\n",
        "        drop_path_rate = dpr[sum(depths[:index]):sum(depths[:index + 1])]\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            SwinTransformerBlock(\n",
        "                dim=dim,\n",
        "                num_heads=num_head,\n",
        "                window_size=window_size,\n",
        "                shift=False if (i % 2 == 0) else True,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=drop_path_rate[i],\n",
        "                norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        if patch_expanding:\n",
        "            self.upsample = PatchExpanding(dim=embed_dim * 2 ** index, norm_layer=norm_layer)\n",
        "        else:\n",
        "            self.upsample = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.blocks:\n",
        "            x = layer(x)\n",
        "        x = self.upsample(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SwinUnet(nn.Module):\n",
        "    def __init__(self, patch_size: int = 4, in_chans: int = 3, num_classes: int = 1000, embed_dim: int = 96,\n",
        "                 window_size: int = 7, depths: tuple = (2, 2, 6, 2), num_heads: tuple = (3, 6, 12, 24),\n",
        "                 mlp_ratio: float = 4., qkv_bias: bool = True, drop_rate: float = 0., attn_drop_rate: float = 0.,\n",
        "                 drop_path_rate: float = 0.1, norm_layer=nn.LayerNorm, patch_norm: bool = True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.window_size = window_size\n",
        "        self.depths = depths\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = len(depths)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.qkv_bias = qkv_bias\n",
        "        self.drop_rate = drop_rate\n",
        "        self.attn_drop_rate = attn_drop_rate\n",
        "        self.drop_path = drop_path_rate\n",
        "        self.norm_layer = norm_layer\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(\n",
        "            patch_size=patch_size, in_c=in_chans, embed_dim=embed_dim,\n",
        "            norm_layer=norm_layer if patch_norm else None)\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "        self.layers = self.build_layers()\n",
        "        self.first_patch_expanding = PatchExpanding(dim=embed_dim * 2 ** (len(depths) - 1), norm_layer=norm_layer)\n",
        "        self.layers_up = self.build_layers_up()\n",
        "        self.skip_connection_layers = self.skip_connection()\n",
        "        self.norm_up = norm_layer(embed_dim)\n",
        "        self.final_patch_expanding = FinalPatchExpanding(dim=embed_dim, norm_layer=norm_layer)\n",
        "        self.head = nn.Conv2d(in_channels=embed_dim, out_channels=num_classes, kernel_size=(1, 1), bias=False)\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def build_layers(self):\n",
        "        layers = nn.ModuleList()\n",
        "        for i in range(self.num_layers):\n",
        "            layer = BasicBlock(\n",
        "                index=i,\n",
        "                depths=self.depths,\n",
        "                embed_dim=self.embed_dim,\n",
        "                num_heads=self.num_heads,\n",
        "                drop_path=self.drop_path,\n",
        "                window_size=self.window_size,\n",
        "                mlp_ratio=self.mlp_ratio,\n",
        "                qkv_bias=self.qkv_bias,\n",
        "                drop_rate=self.drop_rate,\n",
        "                attn_drop_rate=self.attn_drop_rate,\n",
        "                norm_layer=self.norm_layer,\n",
        "                patch_merging=False if i == self.num_layers - 1 else True)\n",
        "            layers.append(layer)\n",
        "        return layers\n",
        "\n",
        "    def build_layers_up(self):\n",
        "        layers_up = nn.ModuleList()\n",
        "        for i in range(self.num_layers - 1):\n",
        "            layer = BasicBlockUp(\n",
        "                index=i,\n",
        "                depths=self.depths,\n",
        "                embed_dim=self.embed_dim,\n",
        "                num_heads=self.num_heads,\n",
        "                drop_path=self.drop_path,\n",
        "                window_size=self.window_size,\n",
        "                mlp_ratio=self.mlp_ratio,\n",
        "                qkv_bias=self.qkv_bias,\n",
        "                drop_rate=self.drop_rate,\n",
        "                attn_drop_rate=self.attn_drop_rate,\n",
        "                patch_expanding=True if i < self.num_layers - 2 else False,\n",
        "                norm_layer=self.norm_layer)\n",
        "            layers_up.append(layer)\n",
        "        return layers_up\n",
        "\n",
        "    def skip_connection(self):\n",
        "        skip_connection_layers = nn.ModuleList()\n",
        "        for i in range(self.num_layers - 1):\n",
        "            dim = self.embed_dim * 2 ** (self.num_layers - 2 - i)\n",
        "            layer = nn.Linear(dim * 2, dim)\n",
        "            skip_connection_layers.append(layer)\n",
        "        return skip_connection_layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        x_save = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x_save.append(x)\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.first_patch_expanding(x)\n",
        "\n",
        "        for i, layer in enumerate(self.layers_up):\n",
        "            x = torch.cat([x, x_save[len(x_save) - i - 2]], -1)\n",
        "            x = self.skip_connection_layers[i](x)\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.norm_up(x)\n",
        "        x = self.final_patch_expanding(x)\n",
        "\n",
        "        x = rearrange(x, 'B H W C -> B C H W')\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "''')"
      ],
      "metadata": {
        "id": "5JPZUjYWGVJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/swin_mae_inference.py', 'w') as f:\n",
        "    f.write('''\n",
        "import sys\n",
        "sys.path.insert(0, '/content/drive/MyDrive/ie643_course_project_24M1644')\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "from einops import rearrange\n",
        "from swin_unet import PatchEmbedding, BasicBlock, PatchExpanding, BasicBlockUp\n",
        "from utils.pos_embed import get_2d_sincos_pos_embed\n",
        "\n",
        "\n",
        "class SwinMAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Masked Autoencoder with Swin Transformer backbone (CUDA-safe version)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224, patch_size=4, mask_ratio=0.25, in_chans=3,\n",
        "                 decoder_embed_dim=768, norm_pix_loss=False,\n",
        "                 depths=(2, 2, 2, 2), embed_dim=96, num_heads=(3, 6, 12, 24),\n",
        "                 window_size=7, qkv_bias=True, mlp_ratio=4.,\n",
        "                 drop_path_rate=0.1, drop_rate=0., attn_drop_rate=0.,\n",
        "                 norm_layer=partial(nn.LayerNorm, eps=1e-6), patch_norm=True):\n",
        "        super().__init__()\n",
        "        self.mask_ratio = mask_ratio\n",
        "        assert img_size % patch_size == 0\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.patch_size = patch_size\n",
        "        self.norm_pix_loss = norm_pix_loss\n",
        "        self.depths = depths\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.qkv_bias = qkv_bias\n",
        "        self.drop_path = drop_path_rate\n",
        "        self.drop_rate = drop_rate\n",
        "        self.attn_drop_rate = attn_drop_rate\n",
        "        self.norm_layer = norm_layer\n",
        "\n",
        "        # Encoder\n",
        "        self.patch_embed = PatchEmbedding(patch_size=patch_size, in_c=in_chans, embed_dim=embed_dim,\n",
        "                                          norm_layer=norm_layer if patch_norm else None)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim), requires_grad=False)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.layers = self.build_layers()\n",
        "\n",
        "        # Decoder\n",
        "        self.first_patch_expanding = PatchExpanding(dim=decoder_embed_dim, norm_layer=norm_layer)\n",
        "        self.layers_up = self.build_layers_up()\n",
        "        self.norm_up = norm_layer(embed_dim)\n",
        "        self.decoder_pred = nn.Linear(decoder_embed_dim // 8, patch_size ** 2 * in_chans, bias=True)\n",
        "\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.num_patches ** 0.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        nn.init.normal_(self.mask_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    # ---- Patch operations ----\n",
        "    def patchify(self, imgs):\n",
        "        p = self.patch_size\n",
        "        h = w = imgs.shape[2] // p\n",
        "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
        "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
        "        x = x.reshape(imgs.shape[0], h * w, p ** 2 * 3)\n",
        "        return x\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        p = self.patch_size\n",
        "        h = w = int(x.shape[1] ** 0.5)\n",
        "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
        "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
        "        imgs = x.reshape(x.shape[0], 3, h * p, h * p)\n",
        "        return imgs\n",
        "\n",
        "    # ---- Window masking ----\n",
        "    def window_masking(self, x, r=4, remove=False, mask_len_sparse=False):\n",
        "        x = rearrange(x, 'B H W C -> B (H W) C')\n",
        "        B, L, D = x.shape\n",
        "        device = x.device\n",
        "        d = int(L ** 0.5 // r)\n",
        "\n",
        "        noise = torch.rand(B, d ** 2, device=device)\n",
        "        sparse_shuffle = torch.argsort(noise, dim=1)\n",
        "        sparse_restore = torch.argsort(sparse_shuffle, dim=1)\n",
        "        sparse_keep = sparse_shuffle[:, :int(d ** 2 * (1 - self.mask_ratio))]\n",
        "\n",
        "        index_keep_part = (torch.div(sparse_keep, d, rounding_mode='floor') * d * r ** 2 +\n",
        "                           sparse_keep % d * r).long().to(device)\n",
        "        index_keep = index_keep_part\n",
        "        for i in range(r):\n",
        "            for j in range(r):\n",
        "                if i == 0 and j == 0:\n",
        "                    continue\n",
        "                offset = (int(L ** 0.5) * i + j)\n",
        "                index_keep = torch.cat([index_keep, (index_keep_part + offset)], dim=1)\n",
        "\n",
        "        index_all = torch.arange(L, device=device).unsqueeze(0).repeat(B, 1)\n",
        "        index_mask = torch.zeros((B, L - index_keep.shape[-1]), dtype=torch.long, device=device)\n",
        "        for i in range(B):\n",
        "            diff = torch.tensor(\n",
        "                np.setdiff1d(index_all[i].cpu().numpy(), index_keep[i].cpu().numpy(), assume_unique=True),\n",
        "                device=device, dtype=torch.long\n",
        "            )\n",
        "            index_mask[i, :diff.shape[0]] = diff\n",
        "\n",
        "        index_shuffle = torch.cat([index_keep, index_mask], dim=1)\n",
        "        index_restore = torch.argsort(index_shuffle, dim=1)\n",
        "\n",
        "        mask = torch.ones([B, L], device=device)\n",
        "        mask[:, :index_keep.shape[-1]] = 0\n",
        "        mask = torch.gather(mask, dim=1, index=index_restore)\n",
        "\n",
        "        if remove:\n",
        "            x_masked = torch.gather(x, dim=1, index=index_keep.unsqueeze(-1).repeat(1, 1, D))\n",
        "            x_masked = rearrange(x_masked, 'B (H W) C -> B H W C', H=int(x_masked.shape[1] ** 0.5))\n",
        "            return x_masked, mask, sparse_restore\n",
        "        else:\n",
        "            x_masked = torch.clone(x)\n",
        "            for i in range(B):\n",
        "                x_masked[i, index_mask[i], :] = self.mask_token.to(device)\n",
        "            x_masked = rearrange(x_masked, 'B (H W) C -> B H W C', H=int(x_masked.shape[1] ** 0.5))\n",
        "            return x_masked, mask\n",
        "\n",
        "    def window_masking_(self, x, window_arr, r=4, remove=False, mask_len_sparse=False, index=27):\n",
        "        \"\"\"\n",
        "        Device-safe variant used in inference with a fixed window_arr.\n",
        "        \"\"\"\n",
        "        x = rearrange(x, 'B H W C -> B (H W) C')\n",
        "        B, L, D = x.shape\n",
        "        device = x.device\n",
        "        d = int(L ** 0.5 // r)\n",
        "\n",
        "        noise = torch.rand(B, d ** 2, device=device)\n",
        "        sparse_shuffle = torch.argsort(noise, dim=1)\n",
        "        sparse_restore = torch.argsort(sparse_shuffle, dim=1)\n",
        "        sparse_keep = sparse_shuffle[:, :int(d ** 2 * (1 - self.mask_ratio))]\n",
        "\n",
        "        # window_arr exclusion on GPU\n",
        "        arr = list(range(0, 196))\n",
        "        arr2 = sorted(list(set(arr) - set(window_arr)))\n",
        "        sparse_keep = torch.tensor([arr2], dtype=torch.long, device=device).repeat(B, 1)\n",
        "\n",
        "        index_keep_part = (torch.div(sparse_keep, d, rounding_mode='floor') * d * r ** 2 +\n",
        "                           sparse_keep % d * r).long().to(device)\n",
        "        index_keep = index_keep_part\n",
        "        for i in range(r):\n",
        "            for j in range(r):\n",
        "                if i == 0 and j == 0:\n",
        "                    continue\n",
        "                offset = (int(L ** 0.5) * i + j)\n",
        "                index_keep = torch.cat([index_keep, (index_keep_part + offset)], dim=1)\n",
        "\n",
        "        index_all = torch.arange(L, device=device).unsqueeze(0).repeat(B, 1)\n",
        "        index_mask = torch.zeros((B, L - index_keep.shape[-1]), dtype=torch.long, device=device)\n",
        "        for i in range(B):\n",
        "            diff = torch.tensor(\n",
        "                np.setdiff1d(index_all[i].cpu().numpy(), index_keep[i].cpu().numpy(), assume_unique=True),\n",
        "                device=device, dtype=torch.long\n",
        "            )\n",
        "            index_mask[i, :diff.shape[0]] = diff\n",
        "\n",
        "        index_shuffle = torch.cat([index_keep, index_mask], dim=1)\n",
        "        index_restore = torch.argsort(index_shuffle, dim=1)\n",
        "\n",
        "        mask = torch.ones([B, L], device=device)\n",
        "        mask[:, :index_keep.shape[-1]] = 0\n",
        "        mask = torch.gather(mask, dim=1, index=index_restore)\n",
        "\n",
        "        if remove:\n",
        "            x_masked = torch.gather(x, dim=1, index=index_keep.unsqueeze(-1).repeat(1, 1, D))\n",
        "            x_masked = rearrange(x_masked, 'B (H W) C -> B H W C', H=int(x_masked.shape[1] ** 0.5))\n",
        "            return x_masked, mask, sparse_restore\n",
        "        else:\n",
        "            x_masked = torch.clone(x)\n",
        "            for i in range(B):\n",
        "                x_masked[i, index_mask[i], :] = self.mask_token.to(device)\n",
        "            x_masked = rearrange(x_masked, 'B (H W) C -> B H W C', H=int(x_masked.shape[1] ** 0.5))\n",
        "            return x_masked, mask\n",
        "\n",
        "    # ---- Network construction ----\n",
        "    def build_layers(self):\n",
        "        layers = nn.ModuleList()\n",
        "        for i in range(len(self.depths)):\n",
        "            layers.append(\n",
        "                BasicBlock(\n",
        "                    index=i,\n",
        "                    depths=self.depths,\n",
        "                    embed_dim=self.embed_dim,\n",
        "                    num_heads=self.num_heads,\n",
        "                    drop_path=self.drop_path,\n",
        "                    window_size=self.window_size,\n",
        "                    mlp_ratio=self.mlp_ratio,\n",
        "                    qkv_bias=self.qkv_bias,\n",
        "                    drop_rate=self.drop_rate,\n",
        "                    attn_drop_rate=self.attn_drop_rate,\n",
        "                    norm_layer=self.norm_layer,\n",
        "                    patch_merging=(i < len(self.depths) - 1)\n",
        "                )\n",
        "            )\n",
        "        return layers\n",
        "\n",
        "    def build_layers_up(self):\n",
        "        layers_up = nn.ModuleList()\n",
        "        for i in range(len(self.depths) - 1):\n",
        "            layers_up.append(\n",
        "                BasicBlockUp(\n",
        "                    index=i,\n",
        "                    depths=self.depths,\n",
        "                    embed_dim=self.embed_dim,\n",
        "                    num_heads=self.num_heads,\n",
        "                    drop_path=self.drop_path,\n",
        "                    window_size=self.window_size,\n",
        "                    mlp_ratio=self.mlp_ratio,\n",
        "                    qkv_bias=self.qkv_bias,\n",
        "                    drop_rate=self.drop_rate,\n",
        "                    attn_drop_rate=self.attn_drop_rate,\n",
        "                    patch_expanding=(i < len(self.depths) - 2),\n",
        "                    norm_layer=self.norm_layer\n",
        "                )\n",
        "            )\n",
        "        return layers_up\n",
        "\n",
        "    # ---- Forward passes ----\n",
        "    def forward_encoder(self, x, window_arr):\n",
        "        x = self.patch_embed(x)\n",
        "        x, mask = self.window_masking(x, remove=False, mask_len_sparse=False)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x, mask\n",
        "\n",
        "    def forward_decoder(self, x):\n",
        "        x = self.first_patch_expanding(x)\n",
        "        for layer in self.layers_up:\n",
        "            x = layer(x)\n",
        "        x = self.norm_up(x)\n",
        "        x = rearrange(x, 'B H W C -> B (H W) C')\n",
        "        x = self.decoder_pred(x)\n",
        "        return x\n",
        "\n",
        "    def forward_loss(self, imgs, pred, mask):\n",
        "        target = self.patchify(imgs)\n",
        "        if self.norm_pix_loss:\n",
        "            mean = target.mean(dim=-1, keepdim=True)\n",
        "            var = target.var(dim=-1, keepdim=True)\n",
        "            target = (target - mean) / (var + 1.e-6) ** 0.5\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)\n",
        "        loss = (loss * mask).sum() / mask.sum()\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x, window_arr):\n",
        "        latent, mask = self.forward_encoder(x, window_arr)\n",
        "        pred = self.forward_decoder(latent)\n",
        "        loss = self.forward_loss(x, pred, mask)\n",
        "        return loss, pred, mask\n",
        "\n",
        "\n",
        "def swin_mae(**kwargs):\n",
        "    model = SwinMAE(\n",
        "        img_size=224, patch_size=4, in_chans=3,\n",
        "        decoder_embed_dim=768,\n",
        "        depths=(2, 2, 2, 2), embed_dim=96, num_heads=(3, 6, 12, 24),\n",
        "        window_size=7, qkv_bias=True, mlp_ratio=4,\n",
        "        drop_path_rate=0.1, drop_rate=0, attn_drop_rate=0,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "''')"
      ],
      "metadata": {
        "id": "w_wP24NsD3jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interface"
      ],
      "metadata": {
        "id": "bdnxznjMKhvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, time, glob, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import argparse\n",
        "from functools import partial\n",
        "import torch.nn as nn\n",
        "from skimage.segmentation import felzenszwalb\n",
        "from skimage.measure import regionprops\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import gradio as gr\n",
        "import tempfile\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ---- CONFIG - edit these if needed ----\n",
        "WINDOW_SIZE = 32  # sliding window size (gamma)\n",
        "WINDOW_STEP = 32  # sliding step (k)\n",
        "WINDOW_BATCH = 2  # how many windows to batch at once (tune for L4)\n",
        "SEED = 42  # fixed seed\n",
        "MIN_GT_PIXELS = -1  # skip tiny GT slices\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# -------------------------------\n",
        "print(\"Device:\", DEVICE)\n",
        "# reproducible\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "from swin_mae_inference import SwinMAE  # Assuming swin_mae_inference.py is in the same directory\n",
        "\n",
        "# Custom yellow colormap (black -> yellow)\n",
        "YELLOW_MAP = LinearSegmentedColormap.from_list(\"black_to_yellow\", [\"black\", \"yellow\"], N=256)\n",
        "\n",
        "\n",
        "# --- Utility functions ------------------------------------------------------\n",
        "def coordinates_to_patch_indexes(x, y, patch_size=16, image_size=224):\n",
        "    px = x // patch_size\n",
        "    py = y // patch_size\n",
        "    px = min(max(px, 0), image_size // patch_size - 1)\n",
        "    py = min(max(py, 0), image_size // patch_size - 1)\n",
        "    return px, py\n",
        "\n",
        "\n",
        "def sliding_windows_for_image(image_shape, window_size, step):\n",
        "    H, W = image_shape[:2]\n",
        "    for y in range(48, H - window_size + 1, step):\n",
        "        for x in range(25, W - window_size + 1, step):\n",
        "            yield x, y\n",
        "\n",
        "\n",
        "def minmax_normalization_uint8(image):\n",
        "    mn = image.min(); mx = image.max()\n",
        "    if mx == mn:\n",
        "        return np.zeros_like(image, dtype=np.uint8)\n",
        "    out = 255 * ((image - mn) / (mx - mn))\n",
        "    return out.astype(np.uint8)\n",
        "\n",
        "\n",
        "def reconstruction_loss_lp(orig, recon):\n",
        "    if orig.ndim == 3 and orig.shape[2] == 3:\n",
        "        o = orig[..., 0]\n",
        "    else:\n",
        "        o = orig\n",
        "    if recon.ndim == 3 and recon.shape[2] == 3:\n",
        "        r = recon[..., 0]\n",
        "    else:\n",
        "        r = recon\n",
        "    return np.abs(o - r)\n",
        "\n",
        "\n",
        "def compute_auprc(y_pred, y_true):\n",
        "    y_pred = y_pred.flatten()\n",
        "    y_true = y_true.flatten()\n",
        "    try:\n",
        "        auprc = average_precision_score(y_true.astype(int), y_pred)\n",
        "        precisions, recalls, thresholds = precision_recall_curve(y_true.astype(int), y_pred)\n",
        "    except Exception:\n",
        "        auprc = float('nan'); precisions, recalls, thresholds = None, None, None\n",
        "    return auprc, precisions, recalls, thresholds\n",
        "\n",
        "\n",
        "def calculate_confusion_matrix(gt, pred):\n",
        "    tp = np.sum((gt == 1) & (pred == 1))\n",
        "    fp = np.sum((gt == 0) & (pred == 1))\n",
        "    fn = np.sum((gt == 1) & (pred == 0))\n",
        "    return tp, fp, fn\n",
        "\n",
        "\n",
        "def calculate_dice_score(gt, pred):\n",
        "    tp, fp, fn = calculate_confusion_matrix(gt, pred)\n",
        "    denom = (2 * tp + fp + fn)\n",
        "    dice = (2 * tp) / denom if denom > 0 else 0.0\n",
        "    return dice, tp, fp, fn\n",
        "\n",
        "\n",
        "# --- run_one_batch_of_windows -----------------------------------------------\n",
        "def run_windows_batched(image_rgb, window_arrs, model, device):\n",
        "    \"\"\"\n",
        "    image_rgb: np.array (H,W,3), values in float (0..1)\n",
        "    window_arrs: list of lists of patch indices for each window\n",
        "    Returns: list of reconstructed images (H,W,3) for each window (same order)\n",
        "\n",
        "    This function tries to call model(x, window_arr) if supported by the loaded model class.\n",
        "    If not supported, it falls back to model(x) (note: fallback DOES NOT apply window-specific masking).\n",
        "    \"\"\"\n",
        "    recon_results = []\n",
        "    x_single = torch.from_numpy(image_rgb[np.newaxis]).float().permute(0, 3, 1, 2).to(device)  # (1,3,H,W)\n",
        "\n",
        "    for arr in window_arrs:\n",
        "        with torch.no_grad():\n",
        "            # prefer calling model(x, arr) if possible (the inference variant).\n",
        "            try:\n",
        "                loss_pred_mask = model(x_single, arr)   # expecting (loss, pred, mask)\n",
        "                # if model returns a single tensor (some variants), handle below\n",
        "            except TypeError:\n",
        "                # fallback: model probably only accepts (x,)\n",
        "                loss_pred_mask = model(x_single)\n",
        "\n",
        "            # normalize all possible return shapes:\n",
        "            # common expected: (loss, pred, mask) where pred shape (N, L, p*p*3)\n",
        "            if isinstance(loss_pred_mask, tuple) or isinstance(loss_pred_mask, list):\n",
        "                _, pred, _ = loss_pred_mask\n",
        "            else:\n",
        "                # model returned only pred (or only output); assume it's pred\n",
        "                pred = loss_pred_mask\n",
        "\n",
        "            # reconstruct image\n",
        "            recon_t = model.unpatchify(pred)                 # (1,3,H,W) or similar\n",
        "            recon_np = recon_t.detach().cpu().numpy()[0]     # (3,H,W)\n",
        "            recon_hw3 = np.transpose(recon_np, (1, 2, 0))\n",
        "            recon_results.append(recon_hw3)\n",
        "\n",
        "            # free\n",
        "            del pred, recon_t, recon_np\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return recon_results\n",
        "# --- main evaluation per-slice using sliding windows -------------------------\n",
        "def evaluate_slice_with_sliding_window(image_rgb, label_mask, model,\n",
        "                                       window_size=WINDOW_SIZE, step=WINDOW_STEP,\n",
        "                                       window_batch=WINDOW_BATCH, device=DEVICE,\n",
        "                                       save_dir=None, base_name=\"subj\", slice_idx=0):\n",
        "    \"\"\"\n",
        "    Compute combined heatmap for a single slice using sliding windows.\n",
        "    Returns: combined_heatmap (H, W) float32 in [0..1].\n",
        "    Also returns a 'pseudo reconstruction' (see note).\n",
        "    \"\"\"\n",
        "    H, W = image_rgb.shape[:2]\n",
        "    img_float = image_rgb.astype(np.float32)\n",
        "    if img_float.max() > 1.0:\n",
        "        img_float = img_float / 255.0\n",
        "\n",
        "    sum_heat = np.zeros((H, W), dtype=np.float32)\n",
        "    coverage = np.zeros((H, W), dtype=np.float32)\n",
        "\n",
        "    windows = [(x, y) for (x, y) in sliding_windows_for_image((H, W), window_size, step)]\n",
        "    window_patch_idx_list = []\n",
        "    for (x, y) in windows:\n",
        "        patch_idxs = []\n",
        "        for a in range(x, x + window_size):\n",
        "            for b in range(y, y + window_size):\n",
        "                px, py = coordinates_to_patch_indexes(a, b, patch_size=16, image_size=224)\n",
        "                final_patch_number = px * (224 // 16) + py\n",
        "                patch_idxs.append(final_patch_number)\n",
        "        window_patch_idx_list.append(list(np.unique(patch_idxs)))\n",
        "\n",
        "    i = 0\n",
        "    N = len(window_patch_idx_list)\n",
        "    last_recon_full = None\n",
        "    while i < N:\n",
        "        batch_arrs = window_patch_idx_list[i:i + window_batch]\n",
        "        batch_windows = windows[i:i + window_batch]\n",
        "\n",
        "        recon_list = run_windows_batched(img_float, batch_arrs, model, device)\n",
        "\n",
        "        for j, (x, y) in enumerate(batch_windows):\n",
        "            recon_win = recon_list[j][y:y + window_size, x:x + window_size, :]\n",
        "            orig_win = img_float[y:y + window_size, x:x + window_size, :]\n",
        "\n",
        "            o = orig_win[..., 0].astype(np.float32)\n",
        "            r = recon_win[..., 0].astype(np.float32)\n",
        "\n",
        "            loss_map = np.abs(o - r).astype(np.float32)\n",
        "            sum_heat[y:y + window_size, x:x + window_size] += loss_map\n",
        "            coverage[y:y + window_size, x:x + window_size] += 1.0\n",
        "\n",
        "        # keep last recon_list's first returned recon as a convenient \"example\" to visualize full-image reconstruction\n",
        "        # NOTE: this is NOT the stitched full reconstruction; it's just the model output for the last processed window call's input image.\n",
        "        # For a true full-image reconstruction you should run model(x_full, window_arr=None or specific) and unpatchify its pred.\n",
        "        try:\n",
        "            last_recon_full = recon_list[0]  # shape (H,W,3)\n",
        "        except Exception:\n",
        "            last_recon_full = None\n",
        "\n",
        "        del recon_list, batch_arrs\n",
        "        torch.cuda.empty_cache()\n",
        "        i += window_batch\n",
        "\n",
        "    coverage_safe = np.where(coverage == 0, 1.0, coverage)\n",
        "    avg_heat = sum_heat / coverage_safe\n",
        "    avg_heat = avg_heat * (img_float[..., 0] > 0.01)\n",
        "\n",
        "    nonzero_vals = avg_heat[avg_heat > 0]\n",
        "    if nonzero_vals.size > 0:\n",
        "        p = np.percentile(nonzero_vals, 99.5)\n",
        "        if p <= 0:\n",
        "            p = nonzero_vals.max() if nonzero_vals.max() > 0 else 1.0\n",
        "    else:\n",
        "        p = 1.0\n",
        "\n",
        "    combined_heatmap = np.clip(avg_heat / p, 0.0, 1.0).astype(np.float32)\n",
        "\n",
        "    # build a pseudo-reconstruction for visualization only (original attenuated by heatmap)\n",
        "    # (This is a stand-in when you don't have a stitched real reconstruction.)\n",
        "    pseudo_recon = img_float.copy()\n",
        "    pseudo_recon[..., 0] = np.clip(img_float[..., 0] * (1.0 - combined_heatmap), 0.0, 1.0)\n",
        "    pseudo_recon = np.stack([pseudo_recon[..., 0]] * 3, axis=-1)\n",
        "\n",
        "    # prefer last_recon_full (model output) if available (not stitched), fallback to pseudo_recon\n",
        "    recon_for_vis = last_recon_full if (last_recon_full is not None) else pseudo_recon\n",
        "\n",
        "    return combined_heatmap, recon_for_vis\n",
        "\n",
        "\n",
        "# --- post-processing & saving visualizations --------------------------------\n",
        "def post_process_and_save(comb_heat_map, org_img, gt, recon_img,\n",
        "                          save_dir, subj_name, slice_num,\n",
        "                          heatmap_dpi=140, heatmap_figsize=(12, 4),\n",
        "                          compact_dpi=140, compact_figsize=(12, 4)):\n",
        "    \"\"\"\n",
        "    Save:\n",
        "     - <subj>_slice_<NNN>_anomaly.png        : Atropos seg | Combined heatmap (yellow) | Final anomaly segmentation\n",
        "    And return (gt_u8, anomaly_mask).\n",
        "\n",
        "    NOTE: The earlier 'reconstruction' PNG is intentionally NOT saved (to save space).\n",
        "    \"\"\"\n",
        "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
        "    base = f\"{subj_name}_slice_{slice_num:03d}\"\n",
        "\n",
        "    # prepare arrays\n",
        "    org_gray = org_img[..., 0] if org_img.ndim == 3 else org_img\n",
        "    org_gray = org_gray.astype(np.float32)\n",
        "    comb_heat_map = comb_heat_map.astype(np.float32)\n",
        "    gt_u8 = (gt > 0).astype(np.uint8)\n",
        "\n",
        "    # --- Compute Atropos / segmentation or fallback ---\n",
        "    try:\n",
        "        import ants\n",
        "        ants_image = ants.from_numpy(org_gray)\n",
        "        img_ = ants.resample_image(ants_image, (224, 224), 1, 0)\n",
        "        mask = ants.get_mask(img_)\n",
        "        img_seg = ants.atropos(a=img_, m='[0.2,1x1]', c='[2,0]', i='kmeans[4]', x=mask)\n",
        "        img_seg = img_seg['segmentation'].numpy()\n",
        "        # resize back if necessary\n",
        "        if img_seg.shape != org_gray.shape:\n",
        "            import skimage.transform as sktf\n",
        "            img_seg = sktf.resize(img_seg, org_gray.shape, order=0, preserve_range=True).astype(np.int32)\n",
        "    except Exception:\n",
        "        # fallback to felzenszwalb segmentation\n",
        "        img_seg = felzenszwalb(org_gray, scale=75, sigma=0.1, min_size=10)\n",
        "\n",
        "    # --- Final anomaly segmentation logic (same as your pipeline) ---\n",
        "    heat_map_rev = (1 - comb_heat_map) * (org_gray > 0.3)\n",
        "    kernel = np.ones((1, 1), np.uint8)\n",
        "    eroded_image = cv2.morphologyEx((heat_map_rev * 255).astype('uint8'), cv2.MORPH_ERODE, kernel)\n",
        "    eroded_image = (eroded_image / 255) > 0.5\n",
        "    segments_old = felzenszwalb(comb_heat_map, scale=75, sigma=0.8, min_size=100)\n",
        "    segments = eroded_image * segments_old\n",
        "    region_props = regionprops(segments, intensity_image=comb_heat_map)\n",
        "    intensity_sorted_regions = sorted(region_props, key=lambda prop: prop.intensity_mean, reverse=True)\n",
        "    top_regions = intensity_sorted_regions[:7]\n",
        "    predicted_mask_comb = np.zeros_like(gt_u8)\n",
        "    for rr in top_regions:\n",
        "        predicted_mask_comb = predicted_mask_comb + (segments == rr.label)\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    predicted_mask_comb = cv2.morphologyEx(predicted_mask_comb.astype('uint8'), cv2.MORPH_DILATE, kernel)\n",
        "\n",
        "    anomaly_mask = (predicted_mask_comb > 0).astype(np.uint8)\n",
        "\n",
        "    # --- PNG: Anomaly visualization only (reduced size) ---\n",
        "    fig, axes = plt.subplots(1, 3, figsize=heatmap_figsize, dpi=heatmap_dpi)\n",
        "    # Atropos / segmentation\n",
        "    axes[0].imshow(org_gray, cmap='gray')\n",
        "    axes[0].imshow(img_seg, alpha=0.5, cmap='nipy_spectral')\n",
        "    axes[0].set_title(\"Atropos / segmentation\"); axes[0].axis('off')\n",
        "    # Combined heatmap with yellow colormap\n",
        "    axes[1].imshow(org_gray, cmap='gray')\n",
        "    axes[1].imshow(comb_heat_map, cmap=YELLOW_MAP, alpha=0.7)\n",
        "    axes[1].set_title(\"Combined Heatmap\"); axes[1].axis('off')\n",
        "    # Final anomaly segmentation overlay\n",
        "    axes[2].imshow(org_gray, cmap='gray')\n",
        "    axes[2].imshow(anomaly_mask, alpha=0.5, cmap='hot')\n",
        "    axes[2].set_title(\"Anomaly segmentation\"); axes[2].axis('off')\n",
        "\n",
        "    anomaly_png = os.path.join(save_dir, f\"{base}_anomaly.png\")\n",
        "    plt.tight_layout(pad=0)\n",
        "    fig.savefig(anomaly_png, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close(fig)\n",
        "\n",
        "    return gt_u8, anomaly_mask\n",
        "\n",
        "\n",
        "# --- run for single pair ----------------------------------------------------\n",
        "def process_single(image_path, seg_path, checkpoint_path, save_dir):\n",
        "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Instantiate & load model\n",
        "    model = SwinMAE(norm_layer=partial(nn.LayerNorm, eps=1e-6), patch_norm=True)\n",
        "    torch.serialization.add_safe_globals([argparse.Namespace])\n",
        "    ck = torch.load(checkpoint_path, map_location='cpu')\n",
        "    if isinstance(ck, dict) and 'model' in ck:\n",
        "        st = ck['model']\n",
        "    elif isinstance(ck, dict) and 'state_dict' in ck:\n",
        "        st = ck['state_dict']\n",
        "    else:\n",
        "        st = ck\n",
        "    new_st = {}\n",
        "    for k, v in st.items():\n",
        "        nk = k[len('module.'):] if k.startswith('module.') else k\n",
        "        new_st[nk] = v\n",
        "    model.load_state_dict(new_st, strict=False)\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    subj = Path(image_path).stem\n",
        "    print(f\"\\n>>> Subject: {subj}\")\n",
        "    if not os.path.exists(image_path) or not os.path.exists(seg_path):\n",
        "        raise ValueError(\"Missing files\")\n",
        "\n",
        "    with h5py.File(image_path, 'r') as f_img, h5py.File(seg_path, 'r') as f_seg:\n",
        "        img_ds = f_img['image']  # shape (1,H,W,D)\n",
        "        seg_key = 'label' if 'label' in f_seg else ('prediction' if 'prediction' in f_seg else None)\n",
        "        if seg_key is None:\n",
        "            raise ValueError(\"seg h5 missing 'label' or 'prediction'\")\n",
        "        seg_ds = f_seg[seg_key]\n",
        "        _, H, W, D = img_ds.shape\n",
        "\n",
        "        results = {}\n",
        "        for s in range(D):\n",
        "            orig = np.array(img_ds[0, :, :, s], dtype=np.float32)\n",
        "            gt_mask = np.array(seg_ds[0, :, :, s], dtype=np.uint8)\n",
        "            if gt_mask.sum() <= MIN_GT_PIXELS:\n",
        "                continue\n",
        "\n",
        "            img_rgb = np.stack([orig, orig, orig], axis=-1)\n",
        "            if img_rgb.max() > 1.0:\n",
        "                img_rgb = img_rgb / 255.0\n",
        "\n",
        "            t0 = time.time()\n",
        "            combined_heatmap, recon_for_vis = evaluate_slice_with_sliding_window(\n",
        "                img_rgb, gt_mask, model,\n",
        "                window_size=WINDOW_SIZE, step=WINDOW_STEP, window_batch=WINDOW_BATCH, device=DEVICE,\n",
        "                save_dir=save_dir, base_name=subj, slice_idx=s\n",
        "            )\n",
        "            t1 = time.time()\n",
        "\n",
        "            gt_out, pred_mask = post_process_and_save(\n",
        "                comb_heat_map=combined_heatmap,\n",
        "                org_img=img_rgb,\n",
        "                gt=gt_mask,\n",
        "                recon_img=recon_for_vis,\n",
        "                save_dir=save_dir,\n",
        "                subj_name=subj,\n",
        "                slice_num=s\n",
        "            )\n",
        "\n",
        "            dice, tp, fp, fn = calculate_dice_score(gt_out, pred_mask)\n",
        "            auprc, precisions, recalls, _ = compute_auprc(combined_heatmap, gt_out)\n",
        "\n",
        "            vispath = os.path.join(save_dir, f\"{subj}_slice_{s:03d}_compact.png\")\n",
        "            # save smaller compact\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(12, 4), dpi=140)\n",
        "            axes[0].imshow(orig, cmap='gray')\n",
        "            axes[0].set_title(\"Original Slice\"); axes[0].axis('off')\n",
        "            axes[1].imshow(orig, cmap='gray'); axes[1].imshow(gt_out, alpha=0.35, cmap='Reds')\n",
        "            axes[1].set_title(\"Original+GT\"); axes[1].axis('off')\n",
        "            axes[2].imshow(recon_for_vis[..., 0], cmap='gray')\n",
        "            axes[2].set_title(\"Reconstructed\"); axes[2].axis('off')\n",
        "            plt.tight_layout(pad=0); fig.savefig(vispath, bbox_inches='tight', pad_inches=0); plt.close(fig)\n",
        "\n",
        "            results[s] = {\n",
        "                \"dice\": float(dice),\n",
        "                \"tp\": int(tp),\n",
        "                \"fp\": int(fp),\n",
        "                \"fn\": int(fn),\n",
        "                \"auprc\": float(auprc) if np.isfinite(auprc) else None,\n",
        "                \"time_sec\": float(t1 - t0),\n",
        "                \"vis\": vispath,\n",
        "                \"anomaly\": os.path.join(save_dir, f\"{subj}_slice_{s:03d}_anomaly.png\")\n",
        "            }\n",
        "\n",
        "            # cleanup\n",
        "            del combined_heatmap, pred_mask, gt_out, recon_for_vis\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"Done processing.\")\n",
        "    return results, subj, save_dir\n",
        "\n",
        "\n",
        "# --- Gradio functions ---\n",
        "def process_files(image_h5, seg_h5, checkpoint):\n",
        "    if not all([image_h5, seg_h5, checkpoint]):\n",
        "        return gr.update(), \"Please upload all files\", None, None, None\n",
        "\n",
        "    save_dir = tempfile.mkdtemp()\n",
        "    try:\n",
        "        results, subj, save_dir = process_single(image_h5.name, seg_h5.name, checkpoint.name, save_dir)\n",
        "    except Exception as e:\n",
        "        return gr.update(), f\"Error: {str(e)}\", None, None, None\n",
        "\n",
        "    if not results:\n",
        "        return gr.update(), \"No slices processed (all skipped due to small GT)\", None, None, None\n",
        "\n",
        "    slices = sorted(results.keys())\n",
        "    slice_choices = [f\"Slice {s}\" for s in slices]\n",
        "    initial_slice = slice_choices[0]\n",
        "    initial_s = slices[0]\n",
        "\n",
        "    metrics = results[initial_s]\n",
        "    metrics_str = f\"DICE: {metrics['dice']:.4f}\\nAUPRC: {metrics['auprc'] if metrics['auprc'] is not None else 'N/A'}\\nTP: {metrics['tp']}\\nFP: {metrics['fp']}\\nFN: {metrics['fn']}\\nTime: {metrics['time_sec']:.2f} sec\"\n",
        "\n",
        "    state = {\"results\": results, \"subj\": subj, \"save_dir\": save_dir, \"slices\": slices}\n",
        "\n",
        "    return gr.update(choices=slice_choices, value=initial_slice), metrics_str, metrics[\"vis\"], metrics[\"anomaly\"], state\n",
        "\n",
        "\n",
        "def update_slice(selected, state):\n",
        "    if not state or not selected:\n",
        "        return \"No slice selected\", None, None\n",
        "\n",
        "    s_str = selected.split(\" \")[1]\n",
        "    s = int(s_str)\n",
        "\n",
        "    metrics = state[\"results\"][s]\n",
        "    metrics_str = f\"DICE: {metrics['dice']:.4f}\\nAUPRC: {metrics['auprc'] if metrics['auprc'] is not None else 'N/A'}\\nTP: {metrics['tp']}\\nFP: {metrics['fp']}\\nFN: {metrics['fn']}\\nTime: {metrics['time_sec']:.2f} sec\"\n",
        "\n",
        "    return metrics_str, metrics[\"vis\"], metrics[\"anomaly\"]\n",
        "\n",
        "\n",
        "# --- Gradio interface ---\n",
        "with gr.Blocks(title=\"SwinMAE Inference\") as demo:\n",
        "    gr.Markdown(\"# SwinMAE Inferencing Interface\")\n",
        "    gr.Markdown(\"Upload the image .h5, anomaly seg .h5, and model checkpoint .pth. Process to generate visualizations per slice.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        image_h5 = gr.File(label=\"Image .h5 File\")\n",
        "        seg_h5 = gr.File(label=\"Anomaly Seg .h5 File\")\n",
        "        checkpoint = gr.File(label=\"Checkpoint .pth File\")\n",
        "\n",
        "    process_btn = gr.Button(\"Process Files\")\n",
        "\n",
        "    slice_dropdown = gr.Dropdown(label=\"Select Slice\", choices=[], interactive=True)\n",
        "    metrics_text = gr.Textbox(label=\"Slice Metrics\", lines=6)\n",
        "    compact_image = gr.Image(label=\"Compact Visualization (Original | Orig+GT | Recon)\")\n",
        "    anomaly_image = gr.Image(label=\"Anomaly Visualization (Atropos/Seg | Heatmap| Anomaly Seg)\")\n",
        "\n",
        "    state = gr.State()\n",
        "\n",
        "    process_btn.click(\n",
        "        process_files,\n",
        "        inputs=[image_h5, seg_h5, checkpoint],\n",
        "        outputs=[slice_dropdown, metrics_text, compact_image, anomaly_image, state]\n",
        "    )\n",
        "    slice_dropdown.change(\n",
        "        update_slice,\n",
        "        inputs=[slice_dropdown, state],\n",
        "        outputs=[metrics_text, compact_image, anomaly_image]\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "y7VC8klr6ina",
        "outputId": "ebc937e4-1d51-4d0d-e67f-58dc1c1e0281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://de09c94b193f67f010.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://de09c94b193f67f010.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uvpuLkqyAx89"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}